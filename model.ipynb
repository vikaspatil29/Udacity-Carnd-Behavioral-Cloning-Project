{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data imported starts here \n",
      "data size is: 438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.47items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features size 300\n",
      "features shape (300, 18, 80, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 39964.78items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: (300, 18, 80, 1)\n",
      "labels: (300,)\n",
      "train size: 202\n",
      "valid size: 68\n",
      "test size: 30\n",
      "input_shape: (18, 80, 1)\n",
      "features count: 1440\n",
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This python script will preprocess the data images.\n",
    "It will import the images from center, left, and right camera\n",
    "and turn it into numpy arrays. These numpy arrays will be splitted\n",
    "into train and validation sets and saved as pickle.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Paths to folder and label\n",
    "folder_path = \"C:/Users/vikas/Desktop/behaviourial_cloning/training_data\"\n",
    "label_path = \"{}/driving_log.csv\".format(folder_path)\n",
    "\n",
    "\n",
    "### Import data\n",
    "data = []\n",
    "with open(label_path) as F:\n",
    "    reader = csv.reader(F)\n",
    "    for i in reader:\n",
    "        data.append(i) \n",
    "\n",
    "print(\"data imported starts here \")\n",
    "\n",
    "### size of the data\n",
    "data_size = len(data)\n",
    "print(\"data size is:\", data_size)\n",
    "\n",
    "### Emtpy generators for feature and labels\n",
    "features = ()\n",
    "labels = ()\n",
    "\n",
    "### This function will resize the images from front, left and\n",
    "### right camera to 18 x 80 and turn them into lists.\n",
    "### The length of the each list will be 18 x 80 = 1440\n",
    "### j = 0,1,2 corresponds to center, left, right\n",
    "def load_image(data_line, j):\n",
    "    img = plt.imread(data_line[j].strip())[65:135:4,0:-1:4,0]\n",
    "    lis = img.flatten().tolist()\n",
    "    return lis\n",
    "\n",
    "data = data[:100]\n",
    "\n",
    "# For each item in data, convert camera images to single list\n",
    "# and save them into features list.\n",
    "for i in tqdm(range(int(len(data))), unit='items'):\n",
    "    for j in range(3):\n",
    "        features += (load_image(data[i],j),)\n",
    "\n",
    "item_num = len(features)\n",
    "print(\"features size\", item_num)\n",
    "\n",
    "# A single list will be convert back to the original image shapes.\n",
    "# Each list contains 3 images so the shape of the result will be\n",
    "# 54 x 80 where 3 images aligned vertically.\n",
    "features = np.array(features).reshape(item_num, 18, 80, 1)\n",
    "print(\"features shape\", features.shape)\n",
    "\n",
    "### Save labels    \n",
    "for i in tqdm(range(int(len(data))), unit='items'):\n",
    "    for j in range(3):\n",
    "        labels += (float(data[i][3]),)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"features:\", features.shape)\n",
    "print(\"labels:\", labels.shape)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Get randomized datasets for training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    test_size=0.10,\n",
    "    random_state=832289)\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=832289)\n",
    "\n",
    "# Print out shapes of new arrays\n",
    "train_size = X_train.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "valid_size = X_valid.shape[0]\n",
    "input_shape = X_train.shape[1:]\n",
    "features_count = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"valid size:\", valid_size)\n",
    "print(\"test size:\", test_size)\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"features count:\", features_count)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the data for easy access\n",
    "pickle_file = 'C:/Users/vikas/Desktop/behaviourial_cloning/examples/carnd.pickle'\n",
    "stop = False\n",
    "\n",
    "while not stop:\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Saving data to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as pfile:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        'train_dataset': X_train,\n",
    "                        'train_labels': y_train,\n",
    "                        'valid_dataset': X_valid,\n",
    "                        'valid_labels': y_valid,\n",
    "                        'test_dataset': X_test,\n",
    "                        'test_labels': y_test,\n",
    "                    },\n",
    "                    pfile, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "        print('Data cached in pickle file.')\n",
    "        stop = True\n",
    "    else:\n",
    "        print(\"Please use a different file name other than carnd.pickle\")\n",
    "        pickle_file = input(\"Enter: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n",
      "train_features size: (202, 18, 80, 1)\n",
      "train_labels size: (202,)\n",
      "valid_features size: (68, 18, 80, 1)\n",
      "valid_labels size: (68,)\n",
      "test_features size: (30, 18, 80, 1)\n",
      "test_labels size: (30,)\n",
      "(18, 80, 1) input shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"valid\", input_shape=(18, 80, 1...)`\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3))`\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3))`\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 78, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 78, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 76, 8)         1160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 76, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 74, 4)         292       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 74, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 72, 2)         74        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 72, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 36, 2)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 36, 2)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 360)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5776      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,023.0\n",
      "Trainable params: 8,023.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202 samples, validate on 68 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# Import keras deep learning libraries\n",
    "import json\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'C:/Users/vikas/Desktop/behaviourial_cloning/examples/carnd.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    pickle_data = pickle.load(f)\n",
    "    X_train = pickle_data['train_dataset']\n",
    "    y_train = pickle_data['train_labels']\n",
    "    X_valid = pickle_data['valid_dataset']\n",
    "    y_valid = pickle_data['valid_labels']\n",
    "    X_test = pickle_data['test_dataset']\n",
    "    y_test = pickle_data['test_labels']\n",
    "    del pickle_data  # Free up memory\n",
    "\n",
    "# Print shapes of arrays that are imported\n",
    "print('Data and modules loaded.')\n",
    "print(\"train_features size:\", X_train.shape)\n",
    "print(\"train_labels size:\", y_train.shape)\n",
    "print(\"valid_features size:\", X_valid.shape)\n",
    "print(\"valid_labels size:\", y_valid.shape)\n",
    "print(\"test_features size:\", X_test.shape)\n",
    "print(\"test_labels size:\", y_test.shape)\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test  = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_valid /= 255\n",
    "X_test  /= 255\n",
    "X_train -= 0.5\n",
    "X_valid -= 0.5\n",
    "X_test  -= 0.5\n",
    "\n",
    "# This is the shape of the image\n",
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape, 'input shape')\n",
    "\n",
    "# Set the parameters and print out the summary of the model\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "batch_size = 64 # The lower the better\n",
    "nb_classes = 1 # The output is a single digit: a steering angle\n",
    "nb_epoch = 10 # The higher the better\n",
    "\n",
    "# import model and wieghts if exists\n",
    "try:\n",
    "\twith open('model.json', 'r') as jfile:\n",
    "\t    model = model_from_json(json.load(jfile))\n",
    "\n",
    "\t# Use adam and mean squared error for training\n",
    "\tmodel.compile(\"adam\", \"mse\")\n",
    "\n",
    "\t# import weights\n",
    "\tmodel.load_weights('model.h5')\n",
    "\n",
    "\tprint(\"Imported model and weights\")\n",
    "\n",
    "# If the model and weights do not exist, create a new model\n",
    "except:\n",
    "\t# If model and weights do not exist in the local folder,\n",
    "\t# initiate a model\n",
    "\n",
    "\t# number of convolutional filters to use\n",
    "\tnb_filters1 = 16\n",
    "\tnb_filters2 = 8\n",
    "\tnb_filters3 = 4\n",
    "\tnb_filters4 = 2\n",
    "\n",
    "\t# size of pooling area for max pooling\n",
    "\tpool_size = (2, 2)\n",
    "\n",
    "\t# convolution kernel size\n",
    "\tkernel_size = (3, 3)\n",
    "\n",
    "\t# Initiating the model\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\t# Starting with the convolutional layer\n",
    "\t# The first layer will turn 1 channel into 16 channels\n",
    "\tmodel.add(Convolution2D(nb_filters1, kernel_size[0], kernel_size[1],\n",
    "\t                        border_mode='valid',\n",
    "\t                        input_shape=input_shape))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# The second conv layer will convert 16 channels into 8 channels\n",
    "\tmodel.add(Convolution2D(nb_filters2, kernel_size[0], kernel_size[1]))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# The second conv layer will convert 8 channels into 4 channels\n",
    "\tmodel.add(Convolution2D(nb_filters3, kernel_size[0], kernel_size[1]))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# The second conv layer will convert 4 channels into 2 channels\n",
    "\tmodel.add(Convolution2D(nb_filters4, kernel_size[0], kernel_size[1]))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# Apply Max Pooling for each 2 x 2 pixels\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\t# Apply dropout of 25%\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t# Flatten the matrix. The input has size of 360\n",
    "\tmodel.add(Flatten())\n",
    "\t# Input 360 Output 16\n",
    "\tmodel.add(Dense(16))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# Input 16 Output 16\n",
    "\tmodel.add(Dense(16))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# Input 16 Output 16\n",
    "\tmodel.add(Dense(16))\n",
    "\t# Applying ReLU\n",
    "\tmodel.add(Activation('relu'))\n",
    "\t# Apply dropout of 50%\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\t# Input 16 Output 1\n",
    "\tmodel.add(Dense(nb_classes))\n",
    "\n",
    "# Print out summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Compile model using Adam optimizer \n",
    "# and loss computed by mean squared error\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### Model training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "import json\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "# Save the model.\n",
    "# If the model.json file already exists in the local file,\n",
    "# warn the user to make sure if user wants to overwrite the model.\n",
    "if 'model.json' in os.listdir():\n",
    "\tprint(\"The file already exists\")\n",
    "\tprint(\"Want to overwite? y or n\")\n",
    "\tuser_input = input()\n",
    "\n",
    "\tif user_input == \"y\":\n",
    "\t\t# Save model as json file\n",
    "\t\tjson_string = model.to_json()\n",
    "\n",
    "\t\twith open('C:/Users/vikas/Desktop/behaviourial_cloning/examples/model.json', 'w') as outfile:\n",
    "\t\t\tjson.dump(json_string, outfile)\n",
    "\n",
    "\t\t\t# save weights\n",
    "\t\t\tmodel.save_weights('.C:/Users/vikas/Desktop/behaviourial_cloning/examples/model.h5')\n",
    "\t\t\tprint(\"Overwrite Successful\")\n",
    "\telse:\n",
    "\t\tprint(\"the model is not saved\")\n",
    "else:\n",
    "\t# Save model as json file\n",
    "\tjson_string = model.to_json()\n",
    "\n",
    "\twith open('model.json', 'w') as outfile:\n",
    "\t\tjson.dump(json_string, outfile)\n",
    "\n",
    "\t\t# save weights\n",
    "\t\tmodel.save_weights('.C:/Users/vikas/Desktop/behaviourial_cloning/examples/model.h5')\n",
    "\t\tprint(\"Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
